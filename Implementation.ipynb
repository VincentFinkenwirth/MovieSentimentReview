{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#  Movie Sentiment Analysis - NLP Project\n",
    "\n",
    "## Introduction\n",
    "With the rise of online reviews, analyzing movie sentiments has become essential for understanding audience feedback. This project aims to classify movie reviews as **positive** or **negative** using a variety of **machine learning**  models.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Information\n",
    "Use **two datasets** to train and evaluate the models:\n",
    "1. **IMDB Movie Reviews Dataset** (50,000 labeled reviews)\n",
    "   - 25,000 **positive** and 25,000 **negative** reviews.\n",
    "   - Source: [Kaggle IMDB Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
    "\n",
    "2. **Rotten Tomatoes Reviews Dataset** (Used for final testing)\n",
    "   - Over 1 million reviews available.\n",
    "   - A **subset** of 50,000 reviews (25k positive & 25k negative) is **added to IMDB to create the training-validation-test datasets** for better generalization.\n",
    "   - The **remaining Rotten Tomatoes reviews** are used for final testing.\n",
    "   - Source: [Kaggle Rotten Tomatoes Dataset](https://www.kaggle.com/datasets/andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews?select=rotten_tomatoes_movie_reviews.csv)\n",
    "\n",
    "---\n",
    "\n",
    "## Models Used\n",
    "Explore **three different types of models** for sentiment classification:\n",
    "\n",
    "1. **Classical Machine Learning**(Optimized with Optuna for hyperparameter tuning)\n",
    "   - **Logistic Regression**\n",
    "   - **Support Vector Machine (SVM)**\n",
    "   - **Naïve Bayes**\n",
    "   - **Random Forest**\n",
    "\n",
    "2. **Gradient Boosting**(Optimized with Optuna for hyperparameter tuning)\n",
    "   - **CatBoost** \n",
    "\n",
    "3. **Deep Learning (Transformer)** (no hyperparameter tuning)\n",
    "   - **BERT (Bidirectional Encoder Representations from Transformers)**  \n",
    "   - Fine-tuned for movie sentiment classification.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Summary\n",
    "\n",
    "1) **Data Preprocessing**\n",
    "   - For **classical models + CatBoost**, preprocess the text data:\n",
    "       - Remove **HTML tags**  \n",
    "       - **Lemmatize words** for better understanding\n",
    "       - Remove **stopwords, and special characters**\n",
    "   - For **BERT**:\n",
    "       - use raw text data without preprocessing (only HTML tag removal)\n",
    "\n",
    "2) **Model Training**\n",
    "   - Train **classical models** (Logistic Regression, SVM, Naïve Bayes, Random Forest)  \n",
    "   - Fine-tune **CatBoost** using Optuna hyperparameter tuning  \n",
    "   - Train **BERT transformer**  \n",
    "\n",
    "3) **Evaluation & Results**\n",
    "   - Evaluate models on a separate test dataset based on rotten tomatoes reviews.\n",
    "   - Generate **confusion matrices, ROC curves, and classification reports**\n",
    "   - My first training was done on the IMDB dataset. The results showed low generalization capabilities on the Rotten Tomatoes dataset. Therefore, I combined the IMDB and Rotten Tomatoes datasets to create a more generalized dataset for training and testing. -> Process 1-3 are repeated based on the combined dataset. Final testing is done on the remainder of the Rotten Tomatoes dataset.\n",
    "\n",
    "4) **Conclusion & Insights**\n",
    "   - Compare model performance  \n",
    "   - Discuss strengths & limitations  \n",
    "   - Suggest future improvements  \n",
    "\n",
    "## Goal: \n",
    "   - **Identify** the best-performing model based on accuracy, F1-score, and generalization ability.\n",
    "   - Provide **visualizations** like confiusion matric and ROC.\n",
    "   - Test **generalization capabilities** of different models.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "202c557059ba6c80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code strucuture\n",
    "1) **EDA.ipynb**: Exploratory Data Analysis of the IMDB and Rotten Tomatoes datasets.\n",
    "2) **/tools**\n",
    "    1) **data_preprocess.py**: Preprocessing class created for classical models and (CatBoost).\n",
    "    2) **data_combination.py**: Splitting, combinig and loading data. Including creating test, validation and training sets and applying preprocessing.\n",
    "    3) **vizualizer.py**: Functions for creating visualizations like confusion matricies and ROC curves used in evaluation.\n",
    "3) **/models**\n",
    "    1) **model_finetuning_classical.py**: Hyperparameter tuning for classical models using Optuna and cross validation.\n",
    "    2) **model_finetuning_catboost.py**: Hyperparameter tuning for CatBoost using Optuna and cross validation.\n",
    "    3) **model_finetuning_bert.py**: Training of BERT model.\n",
    "4) **/data**\n",
    "    1) **/preprocessed**: Preprocessed datasets and splits.\n",
    "        1) **/split_data_v1**: Train, validation and test sets based on solely IMDB data\n",
    "        2) **/split_data_v2**: Train, validation and test sets based on IMDB & Rotten tomatoes data.\n",
    "    2) **/test_datasets**: Results of model evaluation.\n",
    "        1) **/results_v1**: \n",
    "            1) **/report**: Classification reports for each model.\n",
    "            2) **metrics.csv** : Overall metrics\n",
    "            3) **model_v1.csv**: Run URL for each model.\n",
    "       2) **/results_v2**:\n",
    "            1) **/confusion_matrix**: Confusion matrix as png for each model.\n",
    "            2) **/report**: Classification report for each model.\n",
    "            3) **/roc**: ROC curve as png for each model.\n",
    "            4) **metrics_v2.csv**: Overall metrics table.\n",
    "   3) **predictions/**:\n",
    "        1) **predictions_v1.csv**: Predictions of each model on the v1 version of evaluation with text token and \"model_pred\" columns.\n",
    "        2) **predictions_v2.csv**: Predictions of each model on the v2 version of evaluation with text token and \"model_pred\" columns.\n",
    "    4) **imdb_data.csv**:\n",
    "    5) **rotten_tomatoes_movie_reviews.csv**:\n",
    "          \n",
    "5) **/archive**: Archived code used for v1 version of model, before optimizations for v2.\n",
    "6) **model_evaluation.py**: Code to predict models on test set and create metrics ln form of e.g. classification reports based on results.\n",
    "7) **Implementation.ipynb**: This summary of the workflow\n",
    "            \n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c670a382a559a99f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis\n",
    "Done in **EDA.ipynb** notebook. The key idea of this basic EDA is getting an understanding of the columns, data types, and missing values in the dataset. Additionally the preprocessor is tested on the dataset to check if it creates empty output and to see the word distributions in the dataset (More information about preprocessing in the next cell). This helps to get an understanding of the amount of tokens required in the vectorizer to cover most of the content.\n",
    "\n",
    "---\n",
    "\n",
    "### Insights:\n",
    "1) **IMDB Dataset**:\n",
    "   - Contains 50,000 movie reviews with labels (positive/negative).\n",
    "   - No missing values in the dataset.\n",
    "   - The dataset is balanced with 25k positive and 25k negative reviews.\n",
    "   - The preprocessing step works well and doesn't create empty outputs.\n",
    "   - 84742 unique words in the dataset (after preprocessing).\n",
    "   - 117 words occur in more than 10% of the reviews.\n",
    "   - 34978 words occur only once in the dataset. Around 41.3% of the unique tokens -> Carry no significant information useful for training.\n",
    "   - 3216 words carry 80% of the information in the dataset. -> Most important for training\n",
    "\n",
    "2) **Rotten Tomatoes Dataset**: \n",
    "   - Contains over 1 million movie reviews.\n",
    "   - Data contains 11 columns with only 2 relevant columns for analysis. (review and scoreSentiment)\n",
    "   - Contains more positive than negative reviews. (about double - 963799 positive vs 481164 negative)\n",
    "   - Some missing values in the dataset, which have to be deleted before training.\n",
    "   - Preprocessing creates 488 empty outputs in the dataset. -> Need to remove these rows before training. These are reviews only containing stopwords or special characters.\n",
    "   - 133846 unique words in the dataset (after preprocessing).\n",
    "   - Only 2 words occur in more than 10% of the reviews. (movie and film)\n",
    "   - 54284 words occur only once in the dataset. Around 40.6% of the unique tokens.\n",
    "   - 4050 words carry 80% of the information in the dataset.\n",
    "   \n",
    "3) **Takeaways**:\n",
    "   - The **IMDB dataset** is balanced and contains more unique words than the Rotten Tomatoes dataset.\n",
    "   - The **Rotten Tomatoes dataset** contains more reviews but is skewed towards positive reviews and has missing values.\n",
    "   - To capture **80%** of the dataset's token distribution, the vectorizer should consider **at least 4000 words**.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24983e44d9a79f07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing\n",
    "Done in **tools/data_preprocess.py**. The preprocessing steps are different for classical models and the transformer model. The classical models require text data to be preprocessed before training and inference. This helps reduce the noise and dimension of the data, as well as removing punctuation, stopwords, and special characters. The preprocessing steps are as follows:\n",
    "Classical machine learning models require text data to be preprocessed before training and inference. This helps reducing the noise and dimension of the data, aswell as removing punctuation, stopwords, and special characters. In my first approach, I used the **NLTK** library for preprocessing. However, the **Spacy** library provides better performance due to parallel processing and more efficient tokenization. Due to the large dataset size, I removed the NLTK preprocessor class. The \"en_core_web_sm\" model is used for lemmatization and stop-word removal.\n",
    "\n",
    "---\n",
    "### Why OOP approach for preprocessing?\n",
    "1) **Reusability**: The preprocessing steps can be reused more easily and even be implemented in a scikit-learn pipeline. It is saved as a .pkl file and can be loaded easily for future use.\n",
    "2) **Allows modularity**: While not used for this project, in theory the preprocessor can be set up to keep stopwords and casing, which might be useful for future implementations.\n",
    "### Preprocessing Steps:\n",
    "1) **Remove HTML Tags**: Remove any HTML tags present in the text data; these tags carry no useful information and can be removed.\n",
    "2) **Lowercase Text**: Convert the text to lowercase for uniformity and reduced dimensionality, as the model won't differentiate between 'Good' and 'good'. Some \"emotion\" might be lost, as sometimes uppercase letters are used to emphasize a word. However, the trade-off is acceptable, as it reduces the dimensionality of the data.\n",
    "3) **Lemmatization**: Lemmatize the words to their base form for better understanding and to reduce the dimensionality of the data. I prefered lemmatization over stemming, as it provides a more robust baseform and reduces dimensionality more effectively. Again this can lead to some loss of information, as the model might not differentiate between 'good' and 'better'. However, the trade-off is acceptable considering the available computing resources.\n",
    "4) **Remove Stopwords**: Remove stopwords to reduce the dimensionality of the data by removing words like \"the\", \"is\", etc. These words carry very little information and make up a significant portion of the data.\n",
    "5) **Remove Special Characters**: Remove special characters like punctuation marks and numbers. Removing numbers is a trade-off, as they might carry some information. However as they are rare and the vectorizers have a maximum features parameter, I decided to remove them. Some special characters like ! or ? might carry some information, but it is hard to quantify this. I considered creating an additional feature for these characters, but this would increase the dimensionality of the data and might not be useful for generalization and understanding sentiment from words, rather than characters.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bf618a0f65580bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training version 1\n",
    "\n",
    "Models were initially trained on the **IMDB dataset**. The dataset was split into training, validation, and test sets with a distribution of 80-10-10. Afterwards these splits were saved in ** data/preprocessed/split_data_v1** as csv files.\n",
    "\n",
    "---\n",
    "\n",
    "### Why save the preprocessed data?\n",
    "**Reusability & Efficiency**: The preprocessed data can be used for different models without the need to preprocess the data again, this is especially useful as **Optuna** requires multiple training runs, more on this in the training steps.\n",
    "**Consistency**: The same data is used for training and testing, which ensures fair comparison between models.\n",
    "Data loading is done by a function found in **tools/data_combination.py**. This function was created to load the data in a consistent way and allow to choose if a validation set is used or not. The function is used and if the X is the **text** or **token** format. This allows BERT and the other models to use the same data loading function.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Training:\n",
    "The classical and CatBoost model training steps are modified in v2 and therefore the training of v1 is found in the **archive** folder. This is as, while the overall code is similar, the smaller dataset allowed for cross validation in hyperparameter tuning, which can provide better results as a traditional train-val-test split. As BERT requires more computing resources, this was not implemented for it and hence v1 and v2 were trained both using the same code (some minor adjustments of hyperparameters may have occured). For the cross validation the validation set was added to the training set, while keeping the test set the same. More information on why this was change can be found in the training steps of v2.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfde2a541afa2e3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorization\n",
    "To transform text data into numerical format for model training, **two vectorization techniques** were used:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. CountVectorizer**  \n",
    "- Converts text into a **sparse matrix of token counts**.  \n",
    "- Captures **word frequency** but does not account for word importance or word order.  \n",
    "\n",
    "### **2. TF-IDF (Term Frequency-Inverse Document Frequency) Vectorizer**  \n",
    "- Assigns weights based on **word frequency** and **rarity across documents**.  \n",
    "- Helps filter out high-frequency but unimportant words.  \n",
    "- Often improves generalization.\n",
    "\n",
    "### **Hyperparameters and Optimization**\n",
    "- **N-gram range (`(1,1)` or `(1,2)`)**:  \n",
    "  - `(1,1)`: Uses single words (unigrams).  \n",
    "  - `(1,2)`: Includes word pairs (bigrams), which capture short phrases .  \n",
    "- **Max Features (4000-10,000)**:  \n",
    "  - Limits vocabulary to the **most informative words**.  \n",
    "  - Range chosen based on **word frequency distribution from EDA**.  \n",
    "\n",
    "### **Why These Two Vectorizers Were Used?**  \n",
    "- **Efficient** for classical models.  \n",
    "- **Easy to implement**.\n",
    "\n",
    "\n",
    "### **Why Word Embeddings Were Not Used?**  \n",
    "- **Incompatible with classical models** (require aggregation, which leads to loss of information).   \n",
    "- **Increase computational cost** without clear performance gains.  \n",
    "- **Mainly used in deep learning** with e.g. LSTM models-> Not used in this project as BERT is a more advanced deep learning model.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c1e24162644b12"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cc0015bff4382b91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classical Machine Learning Models\n",
    "\n",
    "Classical models are trained using **Optuna for hyperparameter tuning** with **Stratified K-Fold Cross-Validation (K=3)** to enhance model robustness. The dataset is transformed using either **TF-IDF** or **CountVectorizer**, which are included as hyperparameters in the optimization process.\n",
    "\n",
    "---\n",
    "### Libraries:\n",
    "- **Optuna**: An optimization library that can optimize hyperparameters efficiently.\n",
    "- **Scikit-learn**: Vectorizer, models, and evaluation.\n",
    "- **Pandas**: For data manipulation.\n",
    "- **MLflow**: For experiment tracking and logging.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Models trained**\n",
    "- **Logistic Regression**:`C` and `penalty`\n",
    "- **Support Vector Classifier (SVC)**: `C`, `kernel`, `gamma`, and `class_weight`\n",
    "- **Naïve Bayes**:`alpha`\n",
    "- **Random Forest**:`n_estimators`, `max_depth`, `min_samples_split`, `criterion`, and `max_features`\n",
    "- **Models** can be selected by setting the **`MODEL`** variable in the **`GLOBAL`** section.\n",
    "\n",
    "### **Training**\n",
    "- **Optuna** is used to optimize hyperparameters over **100 trials**, parallelized for efficiency. (Tunable parameter within GLOBAL variable - `N_TRIALS` and `N_JOBS`) \n",
    "- **Accuracy** is maximized as the optimization metric -> Accuracy as no priority for false positives or negatives.\n",
    "- **Other metrics** like F1-score, precision and Recall logged in mlflow.\n",
    "- **Stratified K-Fold (K=3)** to ensure robust evaluation.\n",
    "- **MLflow** was used for experiment tracking and logging.\n",
    "- The best-performing model from Optuna is retrained on the **entire training set** and evaluated on the **test set** and logged in mlflow\n",
    "- **Confusion matrix** generated for performance visualization for each trial. \n",
    "\n",
    "Training can be found in **`archive/model_finetuning_classical_cv.py`**. \n",
    "\n",
    "### How to use it:\n",
    "1) Adapt global variables at top like `N_TRIALS`, `N_JOBS` and `CROSS_VALIDATION` according to hardware capabilities and time constraints.\n",
    "2) Choose the `MODEL` and `VECTORIZER` variables which is a list of all to be included in the trial.\n",
    "3) Check the `PATHS`.\n",
    "4) Run **main** function.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d37140a29d07bf36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CatBoost Model Training\n",
    "CatBoost is a **gradient boosting** model that can handle **categorical features** and is known for its **robustness** and **generalization** capabilities. The model was trained using **Optuna for hyperparameter tuning** and **Stratified K-Fold Cross-Validation (K=3)**. In the v1 version **TF-IDF** and **CountVectorizer** were used as vectorizers (changed in v2).\n",
    "\n",
    "---\n",
    "### Libraries:\n",
    "- **CatBoost**: A gradient boosting library that can handle categorical features and is known for its robustness and generalization capabilities.\n",
    "- **Optuna**: An optimization library that can optimize hyperparameters efficiently.\n",
    "- **Scikit-learn**: Vectorizer and evaluation.\n",
    "- **Pandas**: For data manipulation.\n",
    "- **MLflow**: For experiment tracking and logging.\n",
    "\n",
    "---\n",
    "\n",
    "### **Training**\n",
    "- **Optuna** is used to optimize hyperparameters over **20 trials**, no parallelization due to GPU usage.\n",
    "- **Only 20 trials** as CUDA out of memory error occured with more trials -> Optimization in v2.\n",
    "- **Training on CPU** is possible but **significantly slower**.\n",
    "- **Hyperparameters** optimized: `learning_rate`, `depth`, `l2_leaf_reg`, and `iterations`.\n",
    "- **Accuracy** is maximized as the optimization metric -> Accuracy as no priority for false positives or negatives.\n",
    "- **Other metrics** like F1-score, precision and Recall logged in mlflow.\n",
    "- **Stratified K-Fold (K=3)** to ensure robust evaluation.\n",
    "- The best-performing model from Optuna is retrained on the **entire training set** and evaluated on the **test set** and logged in mlflow\n",
    "- **Confusion matrix** generated for performance visualization for each trial.\n",
    "\n",
    "Training can be found in **`archive/model_finetuning_catboost_cv.py`**. \n",
    "\n",
    "### How to use it:\n",
    "1) Adapt global variables like `CROSS_VALIDATION`. \n",
    "2) If **GPU** not available set `TASK_TYPE` to **CPU** and increase `N_TRIALS`.\n",
    "3) Check the `PATHS`.\n",
    "4) Run **main** function.\n",
    "5) Not recommended to run v1 of CatBoost due to usage of classical vectorizers which lead to CUDA out of memory error.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aba2af91a8ab9a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT Model Training\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a pretrained **transformer model** that is finetuned for movie sentiment classification. This allows for better generalization and performance compared to classical models.\n",
    "\n",
    "---\n",
    "### Libraries:\n",
    "- **Transformers**: Hugging Face library for transformer models.\n",
    "- **Panadas**: For data manipulation.\n",
    "- **Scikit-learn**: For evaluation.\n",
    "- **Mlflow**: For experiment tracking and logging.\n",
    "\n",
    "---\n",
    "### Training Process\n",
    "- **Pretrained Tokenizer**: Uses **BERT tokenizer (uncased)** to convert text into tokenized sequences.\n",
    "- **Data Formatting**: Converts the dataset into a **Hugging Face Dataset format** for training.\n",
    "- **Data Collator**: Handles padding for efficient batch processing.\n",
    "- **Evaluation Strategy**: Model is evaluated **at the end of each epoch**.\n",
    "- **Autologging**: Logs training metrics and hyperparameters in **MLflow**.\n",
    "- **Early Stopping**: If no improvement occurs over `3` evaluation rounds, training stops.\n",
    "- **Final Model**: Model is saved and logged in **MLflow**.\n",
    "\n",
    "### How to Use It:\n",
    "1) **Ensure GPU availability** for faster training.  \n",
    "2) **Modify training parameters** in `TrainingArguments` if needed.  \n",
    "3) **Run the main function**\n",
    "4) **Results, model artifacts, and performance metrics** will be stored in **MLflow**.  \n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6675f63de39062d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation (v1)\n",
    "The initial testing for v1 models was done in **archive/model_evaluation_outdated_v1.ipynb**. The evaluation process was done on the **Rotten Tomatoes dataset** , which was **sampled** into **200k positive and negative reviews each**. This reduction in dataset size was done to get a quicker overview of model performances while keeping the dataset large enough to be representative. Sampling was done to reduce class bias influencing metrics like accuracy. Evaluation is improved for v2 and will consider the whole dataset and create additional graphics like confusion matricies and ROC curves. The reason for more basic evaluation is to get an understanding of the generalization capabilities of the models before further actions are taken.\n",
    "\n",
    "---\n",
    "### Evaluation steps v1:\n",
    "1) **Load data** and apply preprocessing using the preprocessor.pkl file. \n",
    "2) **Sample** the Rotten Tomatoes dataset to **200k positive and negative reviews**.\n",
    "3) **Load** and predict using classicals and adding prediction column for each model to the dataframe.\n",
    "4) **Load** and predict using CatBoost and adding prediction column to the dataframe.\n",
    "5) **Load** and predict using BERT and adding prediction column to the dataframe.\n",
    "6) After each step the modified dataframe is saved. This data was later moved to **data/test_datasets/results_v1/rotten_predictions_final_v1_low_accuracy.csv**. This file is used for evaluation of the models. As files were moved and methods were changed, the **model_evaluation_outdated_v1.ipynb is not executable**.\n",
    "7) **Evaluation** is done using sklearns metrics and by calculating TP,FN,FP,TN for each model.\n",
    "\n",
    "---\n",
    "### Model accuracy based on IMDB test dataset (retrieved from mlflow UI):\n",
    "- **Logistic Regression**: 0.8932\n",
    "- **Support Vector Machine**: 0.8938\n",
    "- **Naïve Bayes**: 0.8616\n",
    "- **Random Forest**: 0.8488\n",
    "- **CatBoost**: 0.8670\n",
    "- **BERT**: 0.9012 (eval accuracy)\n",
    "\n",
    "\n",
    "## Evaluation of v1 results:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcf1397a902237a4"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Accuracy        F1  Precision    Recall\n",
      "0        rf  0.583337  0.695605   0.548023  0.951970\n",
      "1        nb  0.698935  0.730093   0.661728  0.814210\n",
      "2       svm  0.703409  0.727010   0.673538  0.789705\n",
      "3   log_reg  0.705928  0.723095   0.683333  0.767770\n",
      "4  catboost  0.639903  0.714987   0.591704  0.903164\n",
      "5      bert  0.826008  0.835534   0.792306  0.883750\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from model_evaluation import evaluate_predictions\n",
    "\n",
    "# Load predictions\n",
    "file_path = \"data/predictions/predictions_v1.csv\"\n",
    "save_path = \"evaluation_results/results_v1/\"\n",
    "predictions = pd.read_csv(file_path)\n",
    "# To allow the v2 evaluation method to work on the v1 data, the sentiment column needs to be renamed to label.\n",
    "predictions = predictions.rename(columns={\"sentiment\": \"label\"})\n",
    "\n",
    "# Evaluate predictions without plotting ROC curve (as no probabilities/confidence scores were computed in v1) and confusion matrix.\n",
    "evaluate_predictions(data=predictions, save_path=save_path, plot_roc_curve=False, plot_confusion_matrix=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T16:27:34.934950200Z",
     "start_time": "2025-02-04T16:27:27.494769600Z"
    }
   },
   "id": "d652ce3446d66431"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model evaluation based on v2 function\n",
    "The evaluation in the cell above uses functionality build for later evaluations, therefore parameters like `plot_roc_curve` and `plot_confusion_matrix` are disabled. While confiusion matricies could be plotted, we are more concerned with basic metrics to get an understanding of overall model performance. Additionally the **classification report** based on **sklearn** can be found at **data/test_datasets/results_v1/report/**. This allows to get a **deeper understanding** of model performance.\n",
    "\n",
    "## Evaluation insights:\n",
    "        \n",
    "| Model    | Accuracy  | F1       | Precision | Recall  |\n",
    "|----------|----------|----------|-----------|---------|\n",
    "| rf       | 0.583337 | 0.695605 | 0.548023  | 0.951970 |\n",
    "| nb       | 0.698935 | 0.730093 | 0.661728  | 0.814210 |\n",
    "| svm      | 0.703409 | 0.727010 | 0.673538  | 0.789705 |\n",
    "| log_reg  | 0.705928 | 0.723095 | 0.683333  | 0.767770 |\n",
    "| catboost | 0.639903 | 0.714987 | 0.591704  | 0.903164 |\n",
    "| bert     | 0.826008 | 0.835534 | 0.792306  | 0.883750 |\n",
    "\n",
    "1) As the datset is **balanced** accuracy is a good metric to evaluate the overall model performance.\n",
    "2) **Precision** indicates how many false positives were predicted by the model. (Presision = TP/(TP+FP))\n",
    "3) **Recall** indicates how many true positives were predicted by the model. (Recall = TP/(TP+FN))\n",
    "4) All models have higher recall than precision, indicating that they are better at predicting true positives than avoiding false positives.\n",
    "5) Shows overall tendency to classify reviews as positive.\n",
    "\n",
    "## Model insights\n",
    "1) **BERT** outperforms all other models in terms of accuracy, F1-score, precision, and recall. Expected as BERT is a more advanced model with great generalization capabilities.\n",
    "2) **Logistic Regression**, **SVM**  and **Naïve Bayes** have similar performance, however significantly worse than evaluation on IIMDB test dataset -> Generalization capabilities are low.\n",
    "3) **CatBoost** shows worse performance than expected -> Could be due to use of classical vectorizers instead of CatBoosts own vectorizer.\n",
    "4) **Random Forest** shows the worst performance -> Very likely overfitting on the training data.\n",
    "\n",
    "## Conclusion\n",
    "1) **BERT** is the best model for sentiment analysis based on the evaluation.\n",
    "2) **Classical models** show low generalization capabilities. -> May be overfitting to training data and style of IMDB reviews\n",
    "3) **CatBoost** needs further work on vectorization.\n",
    "4) **All models** perform worse on rotten tomatoes dataset than on IMDB dataset. -> May be due to different writing styles and less clear distinction between positive and negative reviews, as binary labels are extracted from a continuous scale.\n",
    "4) **Augmenting** training dataset with Rotten Tomatoes dataset could improve generalization capabilities and reduce overfitting on IMDB data. -> Done in v2\n",
    "5) **Further hyperparameter tuning** could improve model performance. -> Done in v2\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed902314b4cc11b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## V2 insights and improvements\n",
    "As the evaluation in v1 showed low generalization capabilities of most models. This shows:\n",
    "1) Need for more **diverse data** for training.\n",
    "2) **Hyperparameter tuning** could further improve models and can prevent overfitting.\n",
    "3) **Vectorizer** for CatBoost needs to be changed.\n",
    "\n",
    "---\n",
    "\n",
    "## Data preparation steps:\n",
    "Data preparation steps are done in **tools/data_combination.py**. The steps are as follows:\n",
    "1) Choosing datasets: **IMDB** and **Rotten Tomatoes** datasets are chosen as the rotten tomatoes dataset contains a very diverse range of reviews from different genres and styles. This can help to improve generalization capabilities.\n",
    "2) **Formatting** both datasets to the same format to ensure compatibility. (dropping unnecessary columns from rotten, renaming columns to \"text\" and \"label\" and binary encoding of sentiment)\n",
    "3) Apply **preprocessing** to both datasets to add a **token** column and delete rows with empty tokens.\n",
    "4) **Splitting** a portion **equal** in **size** and **label distribution** from the rotten tomatoes dataset to **add** to the IMDB dataset, to create the new training set.\n",
    "5) **Splitting** the rotten tomatoes dataset into a **test**, **validation** and **training** set. The training set is used to add to the IMDB dataset.\n",
    "6) The **remainder** of the rotten tomatoes dataset is used for **final testing**.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dba11cbc900b1023"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training v2\n",
    "The v2 training is done in **models/**. The training process is similar to v1, but with some key differences:\n",
    "1) **Cross validation** is removed in favour of a **train-val-test split**. This is done to keep training using optuna managable, as cross validation significantly increases training time. Additionally it keeps consistency accross models, as BERT does not use cross validation.\n",
    "2) **Hyperparameter to tune** are adjusted to help prevent overfitting.\n",
    "3) **CatBoost** uses the **Pool** method and its inbuild vectorization methods. It is still trained on tokens instead of raw text. This also allowed for the number of optuna trials to be increased and overall significantly sped up training.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6397ce4520d6f095"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation v2\n",
    "The evaluation is done in **model_evaluation.py**. The evaluation process is similar to v1, but with some key differences:\n",
    "1) Probabilities/confidence scores are computed for all models, allowing for the creation of **ROC curves**. Stored in **data/test_datasets/results_v2/roc**.\n",
    "2) **Confusion matricies** are plotted for all models. Stored in **data/test_datasets/results_v2/confusion_matrices**.\n",
    "3) **Classification reports** are created from the beginning and stored in **data/test_datasets/results_v2/report**.\n",
    "4) The final testing dataset **IS NOT** sampled and the whole dataset is used for evaluation. This is done to get a more accurate representation of the model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Model accuracy based on test set from combined data (retrieved from mlflow UI):\n",
    "- **Logistic Regression**: 0.8154\n",
    "- **Support Vector Machine**: 0.815\n",
    "- **Naïve Bayes**: 0.7955\n",
    "- **Random Forest**: 0.783\n",
    "- **CatBoost**: 0.826\n",
    "- **BERT**: 0.9012 (eval accuracy)\n",
    "\n",
    "All models have lower accuracy on their test set than in v1, which is expected as the models are now evaluated on a larger and more diverse dataset. This also shows that there is less overfitting on the training data.\n",
    "\n",
    "---\n",
    "## Evaluation of v2 results:\n",
    "As evaluation and visualization is done in **model_evaluation.py** the following cells will just load the results and evaluate them.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e37364a9f1c0d741"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     Model  Accuracy        F1  Precision    Recall\n",
      "0           0        rf  0.758662  0.828212   0.799360  0.859225\n",
      "1           1        nb  0.763517  0.822315   0.836931  0.808200\n",
      "2           2       svm  0.754154  0.807971   0.857467  0.763877\n",
      "3           3   log_reg  0.750357  0.804766   0.855244  0.759914\n",
      "4           4  catboost  0.760003  0.811891   0.864986  0.764937\n",
      "5           5      bert  0.834254  0.865359   0.961533  0.786674\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# metrics path for v2\n",
    "metrics_path = \"evaluation_results/results_v2/metrics_v2.csv\"\n",
    "\n",
    "# Load metrics\n",
    "metrics = pd.read_csv(metrics_path)\n",
    "print(metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T14:19:54.997579300Z",
     "start_time": "2025-02-06T14:19:54.731257900Z"
    }
   },
   "id": "8689316b34ffb338"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Overall evaluation insights:\n",
    "| Model    | Accuracy  | F1       | Precision | Recall  |\n",
    "|----------|----------|----------|-----------|---------|\n",
    "| rf       | 0.758662 | 0.828212 | 0.799360  | 0.859225 |\n",
    "| nb       | 0.763517 | 0.822315 | 0.836931  | 0.808200 |\n",
    "| svm      | 0.754154 | 0.807971 | 0.857467  | 0.763877 |\n",
    "| log_reg  | 0.750357 | 0.804766 | 0.855244  | 0.759914 |\n",
    "| catboost | 0.760003 | 0.811891 | 0.864986  | 0.764937 |\n",
    "| bert     | 0.834254 | 0.865359 | 0.961533  | 0.786674 |\n",
    "\n",
    "---\n",
    "These initial results need to be taken with a grain of salt, as the test datset is not balanced and the test datasets differ for both models, more detail analysis on a model basis will follow.\n",
    "1) Overall **accuracy improved** for all models compared to v1.\n",
    "2) **Precision** is significantly higher for all models, indicating that the models are better at avoiding false positives.\n",
    "3) **Recall** is lower for all models, indicating that the models predict a higher rate of false negatives.\n",
    "\n",
    "---\n",
    "### Difference in model performance between v1 and v2 (v2-v1):\n",
    "| Model    | Accuracy  | F1       | Precision | Recall  |\n",
    "|----------|----------|----------|-----------|---------|\n",
    "| rf       | 0.175325 | 0.132607 | 0.251337  | -0.092745 |\n",
    "| nb       | 0.064582 | 0.092222 | 0.175203  | -0.006010 |\n",
    "| svm      | 0.050745 | 0.080961 | 0.183929  | -0.025828 |\n",
    "| log_reg  | 0.044429 | 0.081671 | 0.171911  | -0.007856 |\n",
    "| catboost | 0.120100 | 0.096904 | 0.273282  | -0.138227 |\n",
    "| bert     | 0.008246 | 0.029825 | 0.169227  | -0.097076 |\n",
    "\n",
    "---\n",
    "1) **BERT** shows the smallest difference in accuracy, indicating that it already had good generalization capabilities in v1.\n",
    "2) **Random Forest** shows the largest improvement in accuracy, indicating that it was indeed overfitting in v1.\n",
    "3) **CatBoost** shows a significant improvement in accuracy, showing that the choice of vectorizer in v1, probably reduced performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Model based v2 evaluation results\n",
    "\n",
    "Loading the classification report and visualizations of each model. Quick explanation of the classification report:\n",
    "1) **0** is the negative class and **1** is the positive class.\n",
    "2) **support** indicates the number of samples in each class.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d8f5025b5e78408"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
      "|:-------------|------------:|---------:|-----------:|:----------|\n",
      "| 0            |    0.649855 | 0.547812 |   0.594486 | 427,955   |\n",
      "| 1            |    0.79936  | 0.859225 |   0.828212 | 897,295   |\n",
      "| accuracy     |    0.758662 | 0.758662 |   0.758662 | 0         |\n",
      "| macro avg    |    0.724607 | 0.703518 |   0.711349 | 1,325,250 |\n",
      "| weighted avg |    0.751081 | 0.758662 |   0.752736 | 1,325,250 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Path to classification report\n",
    "path = \"evaluation_results/results_v2/report/rf_classification_report.csv\"\n",
    "\n",
    "# Load classification report\n",
    "report = pd.read_csv(path)\n",
    "report[\"support\"] = report[\"support\"].apply(lambda x: f\"{int(x):,}\")\n",
    "print(report.to_markdown(index=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T14:26:32.554325100Z",
     "start_time": "2025-02-06T14:26:32.541337500Z"
    }
   },
   "id": "1f53caf6bc3deecb"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1f839a96090c353a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification Report Random Forest:\n",
    "\n",
    "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
    "|:-------------|------------:|---------:|-----------:|:----------|\n",
    "| 0            |    0.649855 | 0.547812 |   0.594486 | 427,955   |\n",
    "| 1            |    0.79936  | 0.859225 |   0.828212 | 897,295   |\n",
    "| accuracy     |    0.758662 | 0.758662 |   0.758662 | 0         |\n",
    "| macro avg    |    0.724607 | 0.703518 |   0.711349 | 1,325,250 |\n",
    "| weighted avg |    0.751081 | 0.758662 |   0.752736 | 1,325,250 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e56de07ebac6e5d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix Random Forest:\n",
    "![image](evaluation_results/results_v2/confusion_matrix/rf_confusion_matrix.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb61f95d8a52f6d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ROC Curve Random Forest:\n",
    "![image](evaluation_results/results_v2/roc/rf_roc_curve.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a5ea643bb14a20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary:\n",
    "1) Low negative class recall -> High false negatives.\n",
    "2) Low precision for negative class -> High false positives, but also to be expected due to class imbalance.\n",
    "3) Good performance on positive class -> High precision and recall.\n",
    "4) Macro avg F1-score will be used for comparison with other models to reduce impact of class imbalance.\n",
    "5) AUC of 0.80 (lowest of all models) but still acceptable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef57a906b81c352b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Naïve Bayes:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "806e0d964cbd935b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
      "|:-------------|------------:|---------:|-----------:|:----------|\n",
      "| 0            |    0.624854 | 0.66983  |   0.646561 | 427,955   |\n",
      "| 1            |    0.836931 | 0.8082   |   0.822315 | 897,295   |\n",
      "| accuracy     |    0.763517 | 0.763517 |   0.763517 | 0         |\n",
      "| macro avg    |    0.730893 | 0.739015 |   0.734438 | 1,325,250 |\n",
      "| weighted avg |    0.768446 | 0.763517 |   0.76556  | 1,325,250 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Path to classification report\n",
    "path = \"evaluation_results/results_v2/report/nb_classification_report.csv\"\n",
    "\n",
    "# Load classification report\n",
    "report = pd.read_csv(path)\n",
    "report[\"support\"] = report[\"support\"].apply(lambda x: f\"{int(x):,}\")\n",
    "print(report.to_markdown(index=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T19:19:03.758859200Z",
     "start_time": "2025-02-04T19:19:03.515514500Z"
    }
   },
   "id": "44e81193106bf2db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification Report Naïve Bayes:\n",
    "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
    "|:-------------|------------:|---------:|-----------:|:----------|\n",
    "| 0            |    0.624854 | 0.66983  |   0.646561 | 427,955   |\n",
    "| 1            |    0.836931 | 0.8082   |   0.822315 | 897,295   |\n",
    "| accuracy     |    0.763517 | 0.763517 |   0.763517 | 0         |\n",
    "| macro avg    |    0.730893 | 0.739015 |   0.734438 | 1,325,250 |\n",
    "| weighted avg |    0.768446 | 0.763517 |   0.76556  | 1,325,250 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0953ee9c3be0e3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix Naïve Bayes:\n",
    "![image](evaluation_results/results_v2/confusion_matrix/nb_confusion_matrix.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b96c177dbbb5578"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ROC Curve Naïve Bayes:\n",
    "![image](data/test_datasets/results_v2/roc/nb_roc_curve.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe506074982c9709"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary:\n",
    "1) Better performance on negative class compared to Random Forest -> More true negative and less false negatives.\n",
    "2) More false positives for negative class -> Lower precision and lower positive class recall.\n",
    "3) Slightly worse performance on positive class compared to Random Forest.\n",
    "4) AUC of 0.83 -> Better discrimination than Random Forest."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99d2c837b19dc2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Support Vector Machine:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77d27c68f33a00a6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
      "|:-------------|------------:|---------:|-----------:|:----------|\n",
      "| 0            |    0.597119 | 0.733769 |   0.658428 | 427,955   |\n",
      "| 1            |    0.857467 | 0.763877 |   0.807971 | 897,295   |\n",
      "| accuracy     |    0.754154 | 0.754154 |   0.754154 | 0         |\n",
      "| macro avg    |    0.727293 | 0.748823 |   0.7332   | 1,325,250 |\n",
      "| weighted avg |    0.773394 | 0.754154 |   0.75968  | 1,325,250 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Path to classification report\n",
    "path = \"evaluation_results/results_v2/report/svm_classification_report.csv\"\n",
    "\n",
    "# Load classification report\n",
    "report = pd.read_csv(path)\n",
    "report[\"support\"] = report[\"support\"].apply(lambda x: f\"{int(x):,}\")\n",
    "print(report.to_markdown(index=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T19:20:46.974946600Z",
     "start_time": "2025-02-04T19:20:46.965679400Z"
    }
   },
   "id": "d992bce1c991ef34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification Report Support Vector Machine:\n",
    "\n",
    "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
    "|:-------------|------------:|---------:|-----------:|:----------|\n",
    "| 0            |    0.597119 | 0.733769 |   0.658428 | 427,955   |\n",
    "| 1            |    0.857467 | 0.763877 |   0.807971 | 897,295   |\n",
    "| accuracy     |    0.754154 | 0.754154 |   0.754154 | 0         |\n",
    "| macro avg    |    0.727293 | 0.748823 |   0.7332   | 1,325,250 |\n",
    "| weighted avg |    0.773394 | 0.754154 |   0.75968  | 1,325,250 |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ea34818b761ab24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix Support Vector Machine:\n",
    "![image](evaluation_results/results_v2/confusion_matrix/svm_confusion_matrix.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35de8815264caed1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ROC Curve Support Vector Machine:\n",
    "![image](evaluation_results/results_v2/roc/svm_roc_curve.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b6a95cadb485f3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary:\n",
    "1) Even greater negative class recall -> Less false negatives -> Also high positive class precision.\n",
    "2) Lower positive recall compared to previous models -> More false negatives\n",
    "3) AUC of 0.83 -> Identical to Naïve Bayes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5787ff454c51cad7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Logistic Regression:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce0e8cfab9716a8e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
      "|:-------------|------------:|---------:|-----------:|:----------|\n",
      "| 0            |    0.591971 | 0.73032  |   0.653908 | 427,955   |\n",
      "| 1            |    0.855244 | 0.759914 |   0.804766 | 897,295   |\n",
      "| accuracy     |    0.750357 | 0.750357 |   0.750357 | 0         |\n",
      "| macro avg    |    0.723607 | 0.745117 |   0.729337 | 1,325,250 |\n",
      "| weighted avg |    0.770227 | 0.750357 |   0.75605  | 1,325,250 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Path to classification report\n",
    "path = \"evaluation_results/results_v2/report/log_reg_classification_report.csv\"\n",
    "\n",
    "# Load classification report\n",
    "report = pd.read_csv(path)\n",
    "report[\"support\"] = report[\"support\"].apply(lambda x: f\"{int(x):,}\")\n",
    "print(report.to_markdown(index=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T19:23:11.030628400Z",
     "start_time": "2025-02-04T19:23:11.021432700Z"
    }
   },
   "id": "30064e8cbdebb11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification Report Logistic Regression:\n",
    "\n",
    "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
    "|:-------------|------------:|---------:|-----------:|:----------|\n",
    "| 0            |    0.591971 | 0.73032  |   0.653908 | 427,955   |\n",
    "| 1            |    0.855244 | 0.759914 |   0.804766 | 897,295   |\n",
    "| accuracy     |    0.750357 | 0.750357 |   0.750357 | 0         |\n",
    "| macro avg    |    0.723607 | 0.745117 |   0.729337 | 1,325,250 |\n",
    "| weighted avg |    0.770227 | 0.750357 |   0.75605  | 1,325,250 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a0b083a5aacc558"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix Logistic Regression:\n",
    "![image](evaluation_results/results_v2/confusion_matrix/log_reg_confusion_matrix.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2a37966e732f100"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ROC Curve Logistic Regression:\n",
    "![image](evaluation_results/results_v2/roc/log_reg_roc_curve.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6b53b203c81a505"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary:\n",
    "1) Similar negative class performance to SVM -> High negative class recall and positive class precision.\n",
    "2) Lower positive class recall compared to SVM -> More false negatives.\n",
    "3) AUC of 0.82 -> Slightly less distinguishing than SVM & Naive Bayes.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37bd6c3b3225f97c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## CatBoost:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0503b25aa536fa6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
      "|:-------------|------------:|---------:|-----------:|:----------|\n",
      "| 0            |    0.603339 | 0.749658 |   0.668587 | 427,955   |\n",
      "| 1            |    0.864986 | 0.764937 |   0.811891 | 897,295   |\n",
      "| accuracy     |    0.760003 | 0.760003 |   0.760003 | 0         |\n",
      "| macro avg    |    0.734162 | 0.757298 |   0.740239 | 1,325,250 |\n",
      "| weighted avg |    0.780494 | 0.760003 |   0.765614 | 1,325,250 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Path to classification report\n",
    "path = \"evaluation_results/results_v2/report/catboost_classification_report.csv\"\n",
    "\n",
    "# Load classification report\n",
    "report = pd.read_csv(path)\n",
    "report[\"support\"] = report[\"support\"].apply(lambda x: f\"{int(x):,}\")\n",
    "print(report.to_markdown(index=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T22:52:52.424926700Z",
     "start_time": "2025-02-04T22:52:51.936796500Z"
    }
   },
   "id": "f1460a2aed47e101"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification Report CatBoost:\n",
    "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
    "|:-------------|------------:|---------:|-----------:|:----------|\n",
    "| 0            |    0.603339 | 0.749658 |   0.668587 | 427,955   |\n",
    "| 1            |    0.864986 | 0.764937 |   0.811891 | 897,295   |\n",
    "| accuracy     |    0.760003 | 0.760003 |   0.760003 | 0         |\n",
    "| macro avg    |    0.734162 | 0.757298 |   0.740239 | 1,325,250 |\n",
    "| weighted avg |    0.780494 | 0.760003 |   0.765614 | 1,325,250 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f7bd8d488249db1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix CatBoost:\n",
    "![image](evaluation_results/results_v2/confusion_matrix/catboost_confusion_matrix.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd4a7513e8f9bce5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ROC Curve CatBoost:\n",
    "![image](evaluation_results/results_v2/roc/catboost_roc_curve.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7e101a04ce2e6bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary:\n",
    "1) High negative class recall -> Lowest false negatives so far.\n",
    "2) Positive class recall is relatively similar to the other models except random forest.\n",
    "3) AUC of 0.84 -> Slightly better discrimination."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1ff1afa0604e8a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## BERT:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec1ca8a202f0c024"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
      "|:-------------|------------:|---------:|-----------:|:----------|\n",
      "| 0            |    0.676187 | 0.934014 |   0.784459 | 427,955   |\n",
      "| 1            |    0.961533 | 0.786674 |   0.865359 | 897,295   |\n",
      "| accuracy     |    0.834254 | 0.834254 |   0.834254 | 0         |\n",
      "| macro avg    |    0.81886  | 0.860344 |   0.824909 | 1,325,250 |\n",
      "| weighted avg |    0.869388 | 0.834254 |   0.839234 | 1,325,250 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Path to classification report\n",
    "path = \"evaluation_results/results_v2/report/bert_classification_report.csv\"\n",
    "\n",
    "# Load classification report\n",
    "report = pd.read_csv(path)\n",
    "report[\"support\"] = report[\"support\"].apply(lambda x: f\"{int(x):,}\")\n",
    "print(report.to_markdown(index=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T22:54:24.598569200Z",
     "start_time": "2025-02-04T22:54:24.542209900Z"
    }
   },
   "id": "14198b1cc899fe46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification Report BERT:\n",
    "| Unnamed: 0   |   precision |   recall |   f1-score | support   |\n",
    "|:-------------|------------:|---------:|-----------:|:----------|\n",
    "| 0            |    0.676187 | 0.934014 |   0.784459 | 427,955   |\n",
    "| 1            |    0.961533 | 0.786674 |   0.865359 | 897,295   |\n",
    "| accuracy     |    0.834254 | 0.834254 |   0.834254 | 0         |\n",
    "| macro avg    |    0.81886  | 0.860344 |   0.824909 | 1,325,250 |\n",
    "| weighted avg |    0.869388 | 0.834254 |   0.839234 | 1,325,250 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9c785425a535478"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix BERT:\n",
    "![image](evaluation_results/results_v2/confusion_matrix/bert_confusion_matrix.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c13720f368296bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ROC Curve BERT:\n",
    "![image](evaluation_results/results_v2/roc/bert_roc_curve.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "542e2f29a393d0e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary:\n",
    "1) Very high negative class recall -> Lowest false negatives by a large margin -> Also high positive class precision.\n",
    "2) Positive class recall worse than Naive Bayes and Random Forest -> More false negatives.\n",
    "3) AUC of 0.94 -> Best discrimination of all models.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ed7217d51d23801"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ranking by macro avg F1-score and AUC:\n",
    "\n",
    "---\n",
    "\n",
    "**Why macro avg F1-score?**\n",
    "1) **Balanced evaluation**: Gives equal weight to both classes as there is no preference in avoiding false positives or negatives.\n",
    "2) **Harmonic mean**: F1-score is the harmonic mean of precision and recall, balancing both metrics.\n",
    "\n",
    "\n",
    "**Why AUC?**\n",
    "1) **Discrimination**: Measures how well the model distinguishes between false positives and true positives.\n",
    "2) **Balanced evaluation**: AUC is less sensitive to class imbalance.\n",
    "\n",
    "---\n",
    "Model: F1 (AUC)\n",
    "1) BERT: 0.824909 (0.94)\n",
    "2) CatBoost: 0.740239 (0.84)\n",
    "3) Naive Bayes: 0.734438 (0.83)\n",
    "4) Support Vector Machine: 0.7332 (0.83)\n",
    "5) Logistic Regression: 0.729337 (0.82)\n",
    "6) Random Forest: 0.711349 (0.80)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59259b148be33fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ranking by class recall:\n",
    "---\n",
    "\n",
    "**Why recall per class?**\n",
    "1) Ratio of true positives to actual positives.\n",
    "2) Indicates how well the model predicts each class.\n",
    "3) Important for understanding class-specific performance.\n",
    "4) High recall is means low false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "**Negative class**:\n",
    "1) BERT: 0.934014\n",
    "2) CatBoost: 0.749658\n",
    "3) Support Vector Machine: 0.733769\n",
    "4) Logistic Regression: 0.73032\n",
    "5) Naive Bayes: 0.66983\n",
    "6) Random Forest: 0.547812\n",
    "\n",
    "\n",
    "**Positive class**:\n",
    "1) Random Forest: 0.859225\n",
    "2) Naive Bayes: 0.8082\n",
    "3) BERT: 0.786674\n",
    "4) CatBoost: 0.764937\n",
    "5) Support Vector Machine: 0.763877\n",
    "6) Logistic Regression: 0.759914\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7554fb38497c812e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion:\n",
    "1) **BERT** is the best model for sentiment analysis based on the evaluation -> Accuracy, F1 and AUC -> Expected as BERT is a more advanced model with most generalization capabilities.\n",
    "2) **CatBoost** is slightly better than classical models, but by less than expected.\n",
    "3) **Classical models** showed good performance given the simplicity of the models, but still significantly worse than BERT.\n",
    "4) **Naive Bayes** and **SVM** showed the best performance of the classical models, whereas Naive Bayes is significantly faster to train -> Good choice for simple sentiment analysis tasks.\n",
    "5) **Random Forest** showed the worst performance overall, but the best positive class recall.\n",
    "\n",
    "**BERT** is the preferrable model to choose, if **GPU** is available and **computational time** is no concern. \n",
    "For **fast** and **simple** sentiment analysis tasks, **Naive Bayes** is the best choice.\n",
    "\n",
    "---\n",
    "## Overview of where models were wrong"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8a09c078f45f64e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   wrong_predictions_count   count\n",
      "0                        0  717734\n",
      "1                        1  180945\n",
      "2                        5  109657\n",
      "3                        2  102166\n",
      "4                        3   80993\n",
      "5                        4   75741\n",
      "6                        6   58014\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Path to predicitions\n",
    "path = \"data/test_datasets/results_v2/predictions.csv\"\n",
    "\n",
    "# Load predictions\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Create a counter of total wrong predictions per review\n",
    "# List of prediction columns \n",
    "model_preds = [\"rf_pred\", \"nb_pred\", \"svm_pred\", \"log_reg_pred\", \"catboost_pred\", \"bert_pred\"]\n",
    "\n",
    "# Count total wrong predictions per review\n",
    "data[\"wrong_predictions_count\"] = data[model_preds].ne(data[\"label\"], axis=0).sum(axis=1)\n",
    "\n",
    "# Only keep relevant columns\n",
    "data = data[[\"text\",\"token\", \"label\", \"wrong_predictions_count\"]]\n",
    "\n",
    "# Sort by wrong predictions count\n",
    "data = data.sort_values(by=\"wrong_predictions_count\", ascending=False)\n",
    "\n",
    "# Count how many reviews had a certain amount of wrong predictions\n",
    "wrong_predictions_count = data[\"wrong_predictions_count\"].value_counts().reset_index()\n",
    "\n",
    "print(wrong_predictions_count)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-05T13:42:53.319847300Z",
     "start_time": "2025-02-05T13:42:49.972712900Z"
    }
   },
   "id": "5f90cc46e0ac21cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Most** reviews have no wrong predictions (717734), but a total of **58014** reviews have been predicted **wrong by all models**. Next we will have a short look at them to understand why they were predicted wrong."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "707b78c1553ef6dd"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text  \\\n",
      "501825  By the time the excessively boring car chase r...   \n",
      "638669  Out Of Time is a cop thriller with a great cas...   \n",
      "309961  Spurlock is still inserting himself into stupi...   \n",
      "638679  An overly constructed little thriller that squ...   \n",
      "638678  Dave Collards' script is certainly inventive a...   \n",
      "887430  Grant's script only hints at a sordid past tha...   \n",
      "309973  Spurlock is aware that this entire thing could...   \n",
      "638672      A great way to waste an afternoon or evening.   \n",
      "309979  I can't exactly recommend the film, but I do r...   \n",
      "978687  When it's over I can't help feeling as disappo...   \n",
      "\n",
      "                                                    token  label  \\\n",
      "501825  time excessively boring car chase roll emotion...      1   \n",
      "638669      time cop thriller great cast trouble feel see      0   \n",
      "309961  spurlock insert stupid game style situation ti...      1   \n",
      "638679  overly construct little thriller squeeze fair ...      1   \n",
      "638678  dave collards script certainly inventive compl...      1   \n",
      "887430  grant script hint sordid past flesh need hour ...      1   \n",
      "309973  spurlock aware entire thing turn badly represe...      1   \n",
      "638672                  great way waste afternoon evening      1   \n",
      "309979  exactly recommend film recommend drinking pom ...      0   \n",
      "978687  help feel disappointed cop actually testament ...      1   \n",
      "\n",
      "        wrong_predictions_count  \n",
      "501825                        6  \n",
      "638669                        6  \n",
      "309961                        6  \n",
      "638679                        6  \n",
      "638678                        6  \n",
      "887430                        6  \n",
      "309973                        6  \n",
      "638672                        6  \n",
      "309979                        6  \n",
      "978687                        6  \n"
     ]
    }
   ],
   "source": [
    "# Only keep reviews with 6 wrong predictions\n",
    "wrong_reviews = data[data[\"wrong_predictions_count\"] == 6]\n",
    "\n",
    "# Show first 10 reviews\n",
    "print(wrong_reviews.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-05T13:45:54.116037500Z",
     "start_time": "2025-02-05T13:45:54.107648200Z"
    }
   },
   "id": "b9cd3918db2e39af"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By the time the excessively boring car chase rolled around&#44; I had more or less emotionally checked out&#46; It had become apparent that what I valued in the movie wasn&#8217;t what its makers &#8212; director Tom Gormican and his co-writer Kevin Etten &#8212; valued&#46; 1\n",
      "------\n",
      "Out Of Time is a cop thriller with a great cast. Trouble is, you may feel as if you've seen it all before. 0\n",
      "------\n",
      "Spurlock is still inserting himself into stupid game-show-style situations, but this time his target is so juicy we're inclined to forgive him. 1\n",
      "------\n",
      "An overly constructed little thriller that squeezes a fair amount of suspense out of its far-fetched plot. 1\n",
      "------\n",
      "Dave Collards' script is certainly inventive and complex but its plot threads quickly unravel as the film goes on and its characters are twisted and pulled into increasingly far-fetched scenarios. 1\n",
      "------\n",
      "Grant's script only hints at a sordid past that's never as fleshed out as it needed to be for \"12 Hour Shift\" to earn the emotional undertow of its ending, but Bettis' clenched face helps fill in the rest. 1\n",
      "------\n",
      "Spurlock is aware that this entire thing could very well turn out badly and represent the end of whatever credibility he's built with viewers, but the transparent nature of the film sort of end-runs that. I think he got it right. 1\n",
      "------\n",
      "A great way to waste an afternoon or evening. 1\n",
      "------\n",
      "I can't exactly recommend the film, but I do recommend drinking POM Wonderful. 0\n",
      "------\n",
      "When it's over I can't help feeling as disappointed as the cops must have been -- which might actually be a testament to the film's proficiency. 1\n",
      "------\n",
      "There wasn't a moment in the film when I wasn't involved or even amused by what I saw. But afterward, I was left with a big pile of \"So what?\" 1\n",
      "------\n",
      "I didn't laugh as much as some, but I have to give him credit for continuing to bring a humorous and entertaining viewpoint to non-fiction film with his gimmicky experiments. 0\n",
      "------\n",
      "I'm certain I'm not the only one who'll make this comparison, but Zodiac could be the All the President's Men of serial killer movies. 1\n",
      "------\n",
      "For all its dramatic flaws, Zodiac deserves praise for not choosing the easy route. 0\n",
      "------\n",
      "“Scrooge: A Christmas Carol” isn’t the best addition to the collection of adaptations, not by a long shot. But it’s surely the glitziest. 0\n",
      "------\n",
      "Aesthetic revolutions for bourgeois living. [Full Review in Spanish] 0\n",
      "------\n",
      "If seeing an innovative faith-based story catches your interest you may not mind muddling through a tedious plot that does indeed have some touching moments. 1\n",
      "------\n",
      "It has about as much logic as Iraq apparently has weapons of mass destruction. 1\n",
      "------\n",
      "Forget about \"greatest.\" But this could be the most self-referential movie ever: literally a 90-minute narrative about its own creation. 1\n",
      "------\n",
      "\"Hey, marketing is all around us, man! Like, everywhere you look there are logos and somebody trying to sell us something. Weird.\" 1\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Look at a few full reviews and label\n",
    "for i in range(20):\n",
    "    print(wrong_reviews.iloc[i][\"text\"], wrong_reviews.iloc[i][\"label\"])\n",
    "    print(\"------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-05T13:51:49.268629600Z",
     "start_time": "2025-02-05T13:51:49.260329600Z"
    }
   },
   "id": "cd1fce941c8e5898"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Insights**:\n",
    "After **manually** reading the first 20 reviews, these are my insights:\n",
    "1) **Wrong labels**: Some reviews seem to be **mislabelled**. e.g. “By the time the excessively boring car chase... I had more or less emotionally checked out...” labeled positive\n",
    "2) **Contradictions**: e.g. \"For all its dramatic flaws, Zodiac deserves praise for not choosing the easy route\" labeled negative\n",
    "3) **Sarcasm**: e.g. \"A great way to waste an afternoon or evening.\" labeled positive. This could either be a mislabel or sarcasm.\n",
    "4) **Complexity**: e.g. \"When it's over I can't help feeling as disappointed as the cops must have been -- which might actually be a testament to the film's proficiency.\" labeled positive. This is hard to understand as the feeling of disappointment isnt clear.\n",
    "5) **Mixed sentiment**: e.g. \"I didn't laugh as much as some, but I have to give him credit for continuing to bring a humorous and entertaining viewpoint to non-fiction film with his gimmicky experiments.\" labeled negative.\n",
    "\n",
    "---\n",
    "**Conclusion**:\n",
    "In  a very large dataset it is **expected** to have **mislabelled** reviews. This could be due to **human error** or **ambiguity** in the review. \n",
    "Additionally the **writing style of critics** can be **hard to interpret** for a model, as even I struggle to understand the sentiment of some reviews.\n",
    "As the labels are binary and extracted from a continuous scale, some reviews may just be **hard to classify** into strictly **positive or negative**.\n",
    "Given the complexity of some of these reviews the overall model performance is still **impressive**.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "312513401be1026a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2013f8ff71664558"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
